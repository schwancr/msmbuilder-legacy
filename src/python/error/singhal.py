import IPython
import numpy as np
from msmbuilder.error import MSMError
from msmbuilder import msm_analysis
from msmbuilder import MSMLib
import scipy.sparse
import logging
logger = logging.getLogger(__name__)

ZERO_THRESHOLD = 1E-8

class SinghalError(MSMError):
    """
    This class uses the method developed by Nina Singhal Hinrichs
    described in:

    [1] Hinrichs, N. S. and Pande, V.S. Calculation of the distribution of 
        eigenvalues and eigenvectors in Markovian state models for molecular 
        dynamics. J. Chem. Phys. 2007, 126, 244101.

    Briefly, the methods will calculate standard deviations in the eigenvalues
    as well as the eigenvector entries for a given MSM.
    """

    def __init__(self, counts, tProb=None, force_dense=False, prior=0.0):
        """
        Initialize the object:
        
        Parameters
        ----------
        count : scipy.sparse matrix or np.ndarray
            counts matrix sampled from your data
        tProb : scipy.sparse matrix or np.ndarray, optional
            transition probability matrix generated by MSMBuilder
            if None, then the count matrix will be row-normalized
        force_dense : bool, optional
            to force dense operations, pass True. (default is False)
        prior : float or np.ndarray, optional
            prior to add to the counts matrix. This can either be an
            array corresponding to a different prior for each state (this
            doesn't really make sense...) or a number to add to all states
            
        Notes
        -----
        If the transition probability matrix was generated with the sliding
            window, then the errors will be systematically too small. If 
            possible you should build a model without sliding window and 
            calculate error bars with this model.
        The variables used in the source code match the cited paper wherever
            possible.
        """

        self.counts = counts
        if tProb is None:
            self.tProb = MSMLib.estimate_transition_matrix(counts)
            # above function just normalizes the rows
        else:
            self.tProb = tProb
    
        self.issparse = (scipy.sparse.issparse(self.tProb) 
                         and scipy.sparse.issparse(self.counts))

        self.weights = np.array(self.counts.sum(axis=1)).flatten()
        self.weights += prior

        if force_dense or not self.issparse:
            # If force_dense is True or one of the matrices is dense
            # then do dense operations
            self.issparse = False
            logger.warning("Forcing dense operations. Watch your memory...")

            if scipy.sparse.issparse(self.tProb):
                self.tProb = self.tProb.toarray()

            if scipy.sparse.issparse(self.counts):
                self.counts = self.counts.toarray()

        if self.issparse:
            #self.tProb = self.tProb.tocsr()
            #self.counts = self.counts.tocsr()
            logger.warning("Dense is the only thing that works :(")
            self.issparse = False
            self.tProb = self.tProb.toarray()
            self.counts = self.coutns.toarray()
            
            # most scipy methods prefer csr

    def get_eigenvalue_variances(self, which_eigenvalues=None, 
                                 eigenvalues=None, return_list=False):
        """
        Calculate the variances in the the eigenvalues of the MSM
        
        Parameters
        ----------
        which_eigenvalues : list or int, optional
            calculate the variance in one or more eigenvalues. If None, then
            calculate variances for all eigenvalues.
        return_list : bool, optional
            if True, then the list of contributions to the variance for each
            state is returned. Otherwise the total variance is returned
        
        Returns
        -------
        variances : np.ndarray (1-d)
            the variance for each requested eigenvalue in the order requested
            if return_list == False
        qi_lists : np.ndarray (2-d)
            the contribution to the variance for each state. The rows correspond
            to the requested eigenvalues, and the columns correspond to 
            contributions from each state. NOTE: This has not been normalized 
            by the number of counts. To get the variance you must do this:
            
            >>> sing_err = SinghalError(...)
            >>> variance[i] = np.sum(qi_lists[i] / (sing_err.weights + 1))

            sing_err.weights corresponds to the number of counts in that state
            plus the prior counts specified
        """

        # First step is to calculate the derivatives with respect to each
        # entry in the transition probability matrix       
        # As derived in [1] (see class docstring) we can solve this by 
        # by calculating the LU-decomposition of A = P - lambda I, where P is
        # the transition matrix

        if eigenvalues is None:
            if which_eigenvalues is None:
                raise Exception("Must provide one of eigenvalues or which_eigenvalues")
            else:
                if isinstance(which_eigenvalues, (list, np.ndarray)):
                    which_eigenvalues = np.array(which_eigenvalues).astype(int)
                    which_eigenvalues = which_eigenvalues.flatten()
                elif isinstance(which_eigenvalues, (int, float)):
                    which_eigenvalues = np.array([int(which_eigenvalues)])
                else:
                    raise Exception("Unrecognized type for which_eigenvalues")
                eigenvalues = msm_analysis.get_eigenvectors(self.tProb, 
                    np.max(which_eigenvalues) + 1)[0]
                eigenvalues = eigenvalues[which_eigenvalues]
        else:
            if isinstance(eigenvalues, (list, np.ndarray)):
                eigenvalues = np.array(eigenvalues).flatten()
            elif isinstance(eigenvalues, float):
                eigenvalues = np.array([eigenvalues])
            else:
                raise Exception("Unrecognized type for eigenvalues")

        qi_lists = [] 
        # qi_lists holds the contribution to each eigenvalue's variance per 
        # state. so, qi_lists[m][n] is the contribution of the n'th state to 
        # the m'th eigenvalue's variance requested.
        # NOTE: these are not normalized yet... so:
        # var_of_eval_i = np.sum(qi_lists[i] / (self.weights + 1))
        num_states = self.tProb.shape[0]
        for eigenvalue in eigenvalues:

            dLambda_dT = self.__get_eigenvalue_derivatives__(eigenvalue)
            # dLambda_dT is a matrix corresponding to the derivative of the
            # eigenvalue with respect to all entries in self.tProb
            # this is the s^\lambda matrix in [1]

            qis = []
            # qis stores the contribution to the variance for each state i
            # see note above about qi_lists
            for i in xrange(num_states):
                middle_mat = np.eye(num_states) * self.tProb[i] - \
                                 np.outer(self.tProb[i], self.tProb[i])
                # middle_mat corresponds to the middle term in equation (25)
                # in [1]:
                # q_i = s_i^T [diag(p_i) - p_i p_i^T] s_i
                # q_i = s_i^T [middle_mat] s_i
                # where s_i is the i'th row of dLambda_dT

                qis.append(dLambda_dT[i].dot(middle_mat).dot(dLambda_dT[i]))
                # qi is the contribution of state i to this eigenvalue's 
                # variance
            
            qi_lists.append(np.array(qis))

        qi_lists = np.array(qi_lists)

        if return_list:
            return qi_lists

        return np.sum(qi_lists / (self.weights + 1), axis=1)


    def get_eigenvector_variances(self, which_eigenvectors=None, 
        eigenvalues=None, eigenvectors=None):
        """
        Compute the error in the eigenvectors corresponding to the method
        described by Singhal Hinrichs. J. Chem. Phys. 2007
        (see Class docstring for full citation)

        Parameters
        ----------
        which_eigenvectors : array_like or int, optional
            which eigenvectors to calculate errors for
        eigenvalues : array_like or float, optional
            which eigenvalue to calculate the errors for. If given, there
            needs to be a corresponding vector stored as a column in
            eigenvectors
        eigenvectors : array_like (2d), optional
            which eigenvector to calculate errors for. If given, there needs
            to be a corresponding eigenvalue in eigenvalues

        Notes
        Not Implemented. Sorry.
        """
        
        raise Exception("Not Implemented.")


    def __get_eigenvalue_derivatives__(self, eigenvalue):
        """
        Internal function for doing the calculating necessary for calculating
        errors using the Singhal method.
        
        You should use get_eigenvalue_variances
        """
    
        num_states = self.tProb.shape[0]

        # Abar is (T - \lambda I) where T is the transition matrix
        # L, U are the lower and upper diagoanl matrices such that Abar = LU
        if self.issparse:
            Abar = self.tProb - eigenvalue * scipy.sparse.eye(num_states, num_states)
            # not sure how to make this work yet... There may not be
            # a sparse implementation
            factorized_lu = scipy.sparse.linalg.splu(Abar)
            L, U = factorized_lu.solve()
        else:
            Abar = self.tProb - eigenvalue * np.eye(num_states)
            L_trans, U_trans = scipy.linalg.lu(Abar.T, permute_l=True)
            L = U_trans.T
            U = L_trans.T
            # scipy puts the unit diagonal in the L matrix, but Singhal specify
            # that U has unit diagonal entries

        # CRS: I am fairly confident I am doing this wrong...
        zero_diagonal_index = np.where(np.abs(L.diagonal()) <= ZERO_THRESHOLD)
        #print zero_diagonal_index, L.diagonal()
        zero_diagonal_index = zero_diagonal_index[0][0]
        # this corresponds to the K used in [1]
        # There needs to be a zero entry in L's diagonal because det(Abar) = 0

        # e_K is a column vector corresponding the K'th column of the identity
        # matrix
        e_K = np.zeros((num_states, 1))
        e_K[zero_diagonal_index] = 1.

        zero_col = np.zeros((num_states, 1))
        # x is the solution to the upper diagonal equation:
        # U x = e_K
        # x_a is the solution to the lower diagonal equation:
        # L^T x_a = 0
        # where 0 is a column vector of zeros

        if self.issparse:
            x = scipy.linalg.sparse.spsolve(U, e_K)
            x_a = scipy.linalg.sparse.spsolve(L.T, zero_col)
        else:
            x = scipy.linalg.solve(U, e_K)
            # equation 2 is the null spce of L^T, so we will solve this
            # with SVD:
            U, S, VH = scipy.linalg.svd(L.T) 
            zero_ind = np.where(S <= ZERO_THRESHOLD)
            x_a = VH[zero_ind].flatten()
        #    print zero_ind, zero_diagonal_index
            # pretty sure these should be the same...
            #x_a = scipy.linalg.solve(L.T, zero_col)
            # The above just returns the trivial solution, which isn't 
            # interesting

        #print x_a
        # there are many solutions to the second equation, so Singhal-
        # Hinrichs choose the solutions such that:
        # x_a[K] = 1
        x_a = x_a / x_a[zero_diagonal_index]

        # This may or may not be an array already, but it is not sparse so we 
        # make it an array
        if scipy.sparse.issparse(x_a):
            x_a = x_a.asarray()
        if scipy.sparse.issparse(x):
            x = x.asarray()

        #print x_a.shape, x.shape
        dlambda_dT = np.outer(x_a, x) / np.dot(x_a, x)
        # dlambda_dT is the matrix of derivatives of the particular eigenvalue
        # with respect to the elements of the transition matrix

        return dlambda_dT
        
